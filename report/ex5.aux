\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}1D}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}}{1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A cleaner image emerges before noise is starting to be reconstructed\relax }}{1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:epoch_compare_1d}{{1}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}}{1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A major difference in results with using ADAM instead of SGD can be seen\relax }}{2}\protected@file@percent }
\newlabel{fig:grad_compare_1d}{{2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces As we see, using L1 gets lower maximum, for the best results we used L2.\relax }}{2}\protected@file@percent }
\newlabel{fig:loss_compare_1d}{{3}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces We see that a high learning rate will make it harder to converge , while a too small learning rate will cause convergence to be too slow.\relax }}{3}\protected@file@percent }
\newlabel{fig:learning_rate_1d}{{4}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}PSNR}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}2D}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}}{4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces As we can see a lot of the noise was indeed removed\relax }}{4}\protected@file@percent }
\newlabel{fig:comparing_epoch_noise_2d}{{5}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}}{4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Learning rate of 1e-3 performs better.)\relax }}{5}\protected@file@percent }
\newlabel{fig:learning_rate_2d}{{6}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}PSNR}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}2D with skip-connections}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}}{5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces 2D with skip-connections after 1500 epochs\relax }}{6}\protected@file@percent }
\newlabel{2ds results}{{7}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}PSNR}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}}{6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces PSNR for 10,000 epochs on image no. 5\relax }}{7}\protected@file@percent }
\newlabel{2ds overfit}{{8}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces PSNR using different amount of filters across all layers.\relax }}{7}\protected@file@percent }
\newlabel{resnet_vs_densnet}{{9}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces PSNR - Residual links vs. Concatenation \relax }}{8}\protected@file@percent }
\newlabel{resnet vs densnet}{{10}{8}}
